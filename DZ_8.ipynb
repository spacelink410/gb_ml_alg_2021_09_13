{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DZ_8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODrX5cc1xyCv"
      },
      "source": [
        "# Урок 8. Снижение размерности данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGKTqaFpcvhf"
      },
      "source": [
        "### 1. Можно ли отобрать наиболее значимые признаки с помощью PCA?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsC_RR1Hc1sl"
      },
      "source": [
        "Да, можно. \n",
        "\n",
        "Т.к смысл метода заключается в том, что с каждой главной компонентой связана определённая доля общей дисперсии исходного набора данных, а дисперсия, является мерой изменчивости данных, то она может отражать уровень информативности этих данных.\n",
        "\n",
        "Вдоль некоторых осей исходного пространства признаков изменчивость может быть большой, вдоль других — малой, а вдоль третьих вообще отсутствовать. Чем меньше дисперсия данных вдоль оси, тем менее значим вклад переменной, связанной с данной осью.\n",
        "\n",
        "Следовательно, решая задачу метода главных компонент мы строим новое пространство признаков меньшей размерности. Максимизируя дисперсию мы вактически находим наиболее важные признаки, а посчитав долю дисперсии мы поймем какие признаки наиболее значимые."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF_4V7Dhc6vo"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsclFEIM6x-q"
      },
      "source": [
        "### Загрузим датасет из урока и проведем предварительные преобразования"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plczalZlc7UE",
        "outputId": "a3bee48c-96e6-46fe-e060-aded9fc64b9b"
      },
      "source": [
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# print(X)\n",
        "\n",
        "# Для начала отмасштабируем выборку\n",
        "X_ = X.astype(float)\n",
        "\n",
        "rows, cols = X_.shape\n",
        "\n",
        "# центрирование - вычитание из каждого значения среднего по строке\n",
        "means = X_.mean(0)\n",
        "for i in range(rows):\n",
        "    for j in range(cols):\n",
        "        X_[i, j] -= means[j]\n",
        "\n",
        "# деление каждого значения на стандартное отклонение\n",
        "std = np.std(X_, axis=0)\n",
        "for i in range(cols):\n",
        "    for j in range(rows):\n",
        "        X_[j][i] /= std[i]\n",
        "\n",
        "# Найдем собственные векторы и собственные значения\n",
        " \n",
        "covariance_matrix = X_.T.dot(X_)\n",
        "\n",
        "eig_values, eig_vectors = np.linalg.eig(covariance_matrix)\n",
        "\n",
        "# сформируем список кортежей (собственное значение, собственный вектор)\n",
        "eig_pairs = [(np.abs(eig_values[i]), eig_vectors[:, i]) for i in range(len(eig_values))]\n",
        "\n",
        "# и отсортируем список по убыванию собственных значений\n",
        "# eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "print('Собственные значения в порядке убывания:')\n",
        "for i in eig_pairs:\n",
        "    print(i[0], i[1])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Собственные значения в порядке убывания:\n",
            "437.77467247979916 [ 0.52106591 -0.26934744  0.5804131   0.56485654]\n",
            "137.1045707202104 [-0.37741762 -0.92329566 -0.02449161 -0.06694199]\n",
            "22.013531335697223 [-0.71956635  0.24438178  0.14212637  0.63427274]\n",
            "3.1072254642928456 [ 0.26128628 -0.12350962 -0.80144925  0.52359713]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqscmE7l62uj"
      },
      "source": [
        "### Посчитаем долю дисперсии и кумулятивную долю дисперсии"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J20rMkPRdJie",
        "outputId": "967df6d2-0966-4853-c8fa-7e438c781f2d"
      },
      "source": [
        "eig_sum = sum(eig_values)\n",
        "var_exp = [(i / eig_sum) * 100 for i in sorted(eig_values, reverse=True)]\n",
        "cum_var_exp = np.cumsum(var_exp)\n",
        "print(f'Доля дисперсии, описвыаемая каждой из компонент \\n{var_exp}')\n",
        "\n",
        "# а теперя оценим кумулятивную (то есть накапливаемую) дисперсию при учитывании каждой из компонент\n",
        "print(f'Кумулятивная доля дисперсии по компонентам \\n{cum_var_exp}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Доля дисперсии, описвыаемая каждой из компонент \n",
            "[72.9624454132999, 22.85076178670175, 3.6689218892828723, 0.5178709107154745]\n",
            "Кумулятивная доля дисперсии по компонентам \n",
            "[ 72.96244541  95.8132072   99.48212909 100.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iFejxZv7H6b"
      },
      "source": [
        "Таким образом мы получили долю влияния признаков и можем из отбирать по принципу, чем больше доля, тем пважнее призанк.\n",
        "\n",
        "Из примера видим, что первые 3 принзнака описывают 99% данных, а например, первые 2 описывают почти 96% данных. Таким образом можем взять либо первые 2, либо первые 3 в зависимоти от бизнес-требований"
      ]
    }
  ]
}