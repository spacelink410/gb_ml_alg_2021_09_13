{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DZ_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODrX5cc1xyCv"
      },
      "source": [
        "# Урок 3. Логистическая регрессия. Log Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQjAcwNWxYRY"
      },
      "source": [
        "# подключим необходимые библиотеки\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "from sklearn import datasets\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jdny79jxx6pa"
      },
      "source": [
        "### 1. Измените функцию calc_logloss так, чтобы нули по возможности не попадали в np.log."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArLq9bHE05F0"
      },
      "source": [
        "Можно сделать переобозначение меток класса на -1 и +1. И в этом случае можно использовать функцию ошибок:\n",
        "$$-\\sum^{l}_{i=1} \\text{ln}(1 + exp(\\left \\langle w,x_{i} \\right \\rangle))$$\n",
        "\n",
        "либо для меток класса 0 и 1 использовать:\n",
        "$$ -\\sum^{l}_{i=1} (y_{i} \\text{ln}\\frac{1}{1 + exp(-\\left \\langle w,x_{i} \\right \\rangle)} + (1 - y_{i})\\text{ln} \\frac{exp(-\\left \\langle w,x_{i} \\right \\rangle)}{1 + exp(-\\left \\langle w,x_{i} \\right \\rangle)}).$$\n",
        "\n",
        "В эти двух случаях мы имеем экспоненциальные функции, значения которых больше 0. А сложение с 1 под логарифмом или в знаменателе не даст 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTq-fA-lBLkp"
      },
      "source": [
        "# функцию в этом случае можно изменить на более простую\n",
        "# loss = -1.0 / m * np.log(1 + np.exp(np.dot(w.T, X)))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcyXV71uDkh3"
      },
      "source": [
        "### 2. Подберите аргументы функции eval_model для логистической регрессии таким образом, чтобы log loss был минимальным."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKZbojHdlWSH"
      },
      "source": [
        "\"\"\"\n",
        " Вспомогательные функции\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Сигмоида\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "# функция потерь\n",
        "def log_loss(w, X, y):\n",
        "    m = X.shape[1]\n",
        "\n",
        "    # используем функцию сигмоиды, написанную ранее\n",
        "    A = sigmoid(np.dot(w.T, X)) \n",
        "    \n",
        "    # вероятность отнесения объекта к классу \"+1\"\n",
        "    loss = -1.0 / m * np.sum(y * np.log(A) + (1 - y) * np.log(1 - A))\n",
        "    grad = 1.0 / m * np.dot(X, (A - y).T)\n",
        "    \n",
        "    return loss, grad\n",
        "\n",
        "\n",
        "# Градиентный спуск\n",
        "def optimize(w, X, y, n_iterations, alpha):\n",
        "    # потери будем записывать в список для отображения в виде графика\n",
        "    losses = []\n",
        "    \n",
        "    for i in range(n_iterations):        \n",
        "        loss, grad = log_loss(w, X, y)\n",
        "        w -= alpha * grad\n",
        "\n",
        "        losses.append(loss)\n",
        "        \n",
        "    return w, losses"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0QMVcsrydwY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe942f81-c26d-41ec-ca0a-eed2d6fb1d3f"
      },
      "source": [
        " # Сгенерируем датасет\n",
        "classes = datasets.make_classification(n_samples=1000,\n",
        "                                       n_features=50,\n",
        "                                       n_informative=20,\n",
        "                                       n_redundant=0,\n",
        "                                       n_classes=2,\n",
        "                                       random_state=1)\n",
        " \n",
        " \n",
        "# перемешаем датасет\n",
        "np.random.seed(12)\n",
        "shuffle_index = np.random.permutation(classes[0].shape[0])\n",
        "X, y = classes[0][shuffle_index], classes[1][shuffle_index]\n",
        "\n",
        "# Транспонируем матрицы\n",
        "X_tr = X.T\n",
        "y_tr = y.reshape(1, y.shape[0])\n",
        "\n",
        "y_tr"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
              "        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
              "        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
              "        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
              "        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
              "        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
              "        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n",
              "        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
              "        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
              "        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
              "        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
              "        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
              "        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
              "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
              "        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
              "        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
              "        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
              "        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,\n",
              "        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
              "        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
              "        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
              "        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
              "        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
              "        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
              "        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
              "        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
              "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
              "        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
              "        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
              "        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
              "        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
              "        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,\n",
              "        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
              "        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
              "        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
              "        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
              "        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
              "        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
              "        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
              "        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
              "        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
              "        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
              "        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n",
              "        0, 1, 0, 0, 0, 1, 1, 1, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJDjQscrEdQG",
        "outputId": "a52ebdde-0afe-4409-b668-a95a5214052e"
      },
      "source": [
        "# иницилизируем начальный вектор весов\n",
        "w0 = np.zeros((X_tr.shape[0], 1))\n",
        "\n",
        "n_iterations = 10000\n",
        "alpha = 0.005\n",
        "\n",
        "w, losses = optimize(w0, X_tr, y_tr, n_iterations, alpha)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Итоговый вектор весов w: {w}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Итоговый вектор весов w: [[ 0.0826704 ]\n",
            " [-0.14852567]\n",
            " [ 0.11721495]\n",
            " [-0.46688234]\n",
            " [ 0.20198077]\n",
            " [ 0.01000646]\n",
            " [ 0.08826596]\n",
            " [ 0.06418002]\n",
            " [-0.09234752]\n",
            " [-0.22713773]\n",
            " [ 0.00658675]\n",
            " [ 0.13203703]\n",
            " [ 0.06560031]\n",
            " [-0.04347638]\n",
            " [-0.10568227]\n",
            " [-0.06823765]\n",
            " [-0.24657751]\n",
            " [ 0.09280656]\n",
            " [-0.05160206]\n",
            " [ 0.05680034]\n",
            " [-0.28848955]\n",
            " [-0.07884919]\n",
            " [-0.29207902]\n",
            " [ 0.06885236]\n",
            " [ 0.09987036]\n",
            " [ 0.10915652]\n",
            " [-0.03110719]\n",
            " [-0.03303839]\n",
            " [-0.10287925]\n",
            " [ 0.45402388]\n",
            " [-0.44826863]\n",
            " [-0.0073087 ]\n",
            " [-0.01976011]\n",
            " [ 0.37259946]\n",
            " [ 0.00847365]\n",
            " [-0.03747964]\n",
            " [ 0.12162821]\n",
            " [ 0.03771362]\n",
            " [-0.03894745]\n",
            " [-0.04657441]\n",
            " [ 0.13890005]\n",
            " [ 0.00673495]\n",
            " [-0.3913916 ]\n",
            " [-0.06742779]\n",
            " [-0.11021755]\n",
            " [ 0.08828855]\n",
            " [ 0.14209093]\n",
            " [-0.0407526 ]\n",
            " [ 0.13632125]\n",
            " [-0.27836117]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fYLAXFBWZe3"
      },
      "source": [
        "### 3. Создайте функцию calc_pred_proba, возвращающую предсказанную вероятность класса 1 (на вход подаются W, который уже посчитан функцией eval_model и X, на выходе - массив y_pred_proba)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zP4xZTLgWg-p",
        "outputId": "6739a027-e588-4c36-ed4b-a95e110c9fc7"
      },
      "source": [
        "def calc_pred_proba(w, X):\n",
        "    A = sigmoid(np.dot(w.T, X))\n",
        "    return A\n",
        "\n",
        "print(calc_pred_proba(w, X_tr))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[9.40127463e-01 8.45359668e-03 2.30236065e-02 2.77590704e-01\n",
            "  8.64877046e-01 5.30390549e-01 9.14795685e-01 9.72643767e-01\n",
            "  5.82102674e-02 6.34329881e-01 2.23560992e-02 9.76713858e-01\n",
            "  3.43760883e-02 1.07723069e-02 1.85948545e-03 2.50695673e-02\n",
            "  9.88757756e-01 9.44247610e-01 5.68119826e-01 1.15242532e-01\n",
            "  2.51426111e-01 2.32142600e-01 9.92164896e-01 2.50717288e-01\n",
            "  9.24932403e-01 4.97465301e-02 7.34682054e-01 9.97696838e-01\n",
            "  8.59871239e-01 2.14806550e-01 1.46746736e-04 9.62361364e-01\n",
            "  3.06872108e-01 2.07973935e-02 1.75692613e-01 9.98723804e-01\n",
            "  6.15755197e-01 8.09481242e-02 9.93949926e-01 5.24323712e-01\n",
            "  4.96708973e-02 6.84695988e-01 3.06196981e-02 2.66069046e-02\n",
            "  4.36996554e-02 9.71891492e-01 2.85223382e-01 9.05087022e-01\n",
            "  1.40218460e-02 2.46054349e-02 9.86992384e-01 1.47325814e-02\n",
            "  9.95425545e-01 9.86335273e-01 2.83917595e-01 7.29667839e-01\n",
            "  9.97013794e-01 9.90189143e-01 1.43376190e-01 9.84275181e-01\n",
            "  9.49051594e-01 3.78637452e-02 3.41606853e-01 9.65873368e-01\n",
            "  1.81122684e-01 4.58184418e-02 4.90605904e-01 3.48333125e-01\n",
            "  1.75404914e-03 9.64941659e-01 9.83161733e-01 1.23932161e-01\n",
            "  3.03946810e-03 7.72861193e-01 5.73875266e-01 1.28708911e-01\n",
            "  4.56985908e-01 2.60232304e-02 2.33881252e-02 3.71579523e-01\n",
            "  1.79646125e-02 9.56075202e-01 2.30394048e-01 1.44343963e-02\n",
            "  7.94011193e-01 2.07334966e-02 3.40892543e-02 3.09037628e-01\n",
            "  9.86261388e-01 4.89333453e-04 1.63384644e-01 2.33040341e-03\n",
            "  6.77361408e-02 9.53515156e-01 3.79988727e-01 1.67972944e-01\n",
            "  4.59448931e-01 5.55680389e-02 9.42277168e-01 4.16440924e-01\n",
            "  2.37838594e-01 6.60104807e-01 1.56028656e-03 2.55071890e-02\n",
            "  1.20086499e-01 4.28144968e-01 6.71973761e-01 8.62825899e-01\n",
            "  9.99668387e-01 6.20261568e-01 1.98270513e-01 8.91948013e-01\n",
            "  1.22556937e-01 1.32196124e-01 1.97023697e-01 9.69882300e-01\n",
            "  1.11240514e-02 9.98383626e-01 5.89749310e-01 2.45216393e-01\n",
            "  3.61256790e-01 4.40849949e-01 9.70575657e-01 2.53569464e-01\n",
            "  2.19275930e-01 5.66672275e-02 9.52845137e-01 9.95825138e-01\n",
            "  7.46980759e-02 7.04492180e-01 3.20672684e-01 5.44342880e-02\n",
            "  6.53472129e-01 3.03957469e-02 9.56451795e-01 9.88223540e-01\n",
            "  9.93787645e-01 2.41912738e-01 1.25431380e-01 7.89120465e-01\n",
            "  9.84878787e-01 2.19823982e-02 5.83949411e-01 4.37945532e-03\n",
            "  2.25315498e-01 9.99354583e-01 9.37948027e-02 9.68634197e-01\n",
            "  3.04275224e-01 7.02256994e-01 3.42251746e-02 1.37082826e-01\n",
            "  4.26726078e-01 5.59676369e-01 9.97563658e-01 3.98079809e-02\n",
            "  6.12747159e-01 2.22937387e-02 5.70920932e-02 8.74039551e-02\n",
            "  9.56876233e-01 1.04073771e-02 9.12104229e-01 5.57432076e-03\n",
            "  9.94947160e-01 4.41850864e-01 9.96649593e-01 1.62089236e-01\n",
            "  8.04178630e-03 1.38465339e-01 9.76413429e-01 4.55340678e-01\n",
            "  9.91478236e-01 9.09023278e-01 7.73289031e-01 9.89413548e-01\n",
            "  1.80902188e-02 1.48686180e-01 9.16307007e-01 4.39327706e-02\n",
            "  8.79963202e-01 3.40748726e-02 5.67210112e-01 5.58515685e-01\n",
            "  9.48188930e-01 7.21299659e-01 1.67261191e-01 9.97475292e-01\n",
            "  8.99654504e-01 1.93747724e-02 7.03168390e-01 5.69686748e-02\n",
            "  1.00487804e-03 9.35978973e-01 2.02595701e-02 2.64427676e-01\n",
            "  9.08937565e-01 5.57017843e-01 4.48922258e-03 9.99059766e-01\n",
            "  1.57615631e-01 6.97594230e-01 8.16115047e-01 2.15133002e-01\n",
            "  9.40573697e-02 7.54513726e-02 8.10407524e-01 7.03148683e-02\n",
            "  3.42736731e-01 7.55316717e-04 3.60412737e-04 3.35905316e-01\n",
            "  7.25938240e-02 8.24801882e-01 8.44435033e-01 8.58787828e-01\n",
            "  9.90215584e-01 4.37297530e-03 5.87361043e-02 4.48276237e-02\n",
            "  9.74144043e-01 9.95168191e-01 9.91776826e-01 4.47958232e-03\n",
            "  8.95119039e-01 9.96193274e-01 9.99626048e-01 2.50171747e-03\n",
            "  9.76504947e-01 1.10105111e-01 3.62688706e-02 9.99907203e-01\n",
            "  2.99161313e-01 2.49057828e-01 3.29926184e-02 3.84578175e-01\n",
            "  4.82856894e-01 1.04903169e-02 5.73855224e-02 9.26483745e-04\n",
            "  2.49340737e-01 9.79871640e-01 6.12319517e-02 5.11407611e-01\n",
            "  2.85926973e-02 9.78457066e-01 1.04444319e-02 8.53143303e-01\n",
            "  7.42935091e-01 1.39422846e-01 1.51184304e-01 7.63734394e-01\n",
            "  6.63768120e-01 8.93277732e-01 1.65615160e-02 9.99328992e-01\n",
            "  1.02369534e-01 7.60024885e-02 4.41532382e-01 2.19071409e-01\n",
            "  1.24560965e-01 4.91872208e-01 9.99417982e-01 3.83983775e-01\n",
            "  1.49384272e-02 8.55665402e-01 5.54227256e-02 4.91257509e-02\n",
            "  3.45948348e-01 1.48788147e-01 2.99810030e-01 1.80111635e-01\n",
            "  6.62812038e-01 9.59385103e-01 4.00522485e-02 9.15070289e-01\n",
            "  9.26812809e-01 3.28453647e-01 8.84219247e-01 9.97563648e-01\n",
            "  4.21348815e-01 1.81373665e-01 4.90588203e-01 7.75426456e-02\n",
            "  9.79104640e-01 9.44510862e-01 6.92777073e-01 8.64937435e-01\n",
            "  6.43889746e-01 1.53091084e-02 9.65990098e-01 9.80693810e-01\n",
            "  8.62246625e-03 3.16235885e-01 9.54576780e-01 8.65485219e-01\n",
            "  7.02950404e-02 9.37072335e-01 3.57791582e-01 7.40558360e-01\n",
            "  1.10424006e-01 9.89993014e-01 7.73436510e-01 2.05590733e-01\n",
            "  1.00738435e-01 9.52338883e-01 9.99748854e-01 9.61505718e-01\n",
            "  2.45898471e-01 9.07714624e-01 9.66468529e-01 2.82759353e-02\n",
            "  1.22377017e-01 5.43451347e-01 1.46066023e-01 2.73812574e-01\n",
            "  9.96077983e-01 8.50386590e-01 1.93181589e-02 4.48044505e-01\n",
            "  9.70658605e-01 3.04281621e-01 1.64806377e-01 8.13468337e-03\n",
            "  5.96935281e-01 9.46324386e-01 9.98611726e-01 4.84006463e-02\n",
            "  6.55541866e-01 2.84464732e-01 8.79386232e-01 7.98327411e-01\n",
            "  3.85898907e-03 9.34764309e-01 9.08668389e-02 9.94215907e-01\n",
            "  3.05872362e-03 9.41157537e-01 9.10534374e-02 9.96934925e-01\n",
            "  5.41755592e-05 9.13930259e-01 4.98664539e-02 8.31189975e-04\n",
            "  7.19881612e-01 5.71932131e-02 1.98361609e-01 7.15862423e-01\n",
            "  1.84269151e-01 5.30110641e-01 5.35648300e-01 9.71131596e-01\n",
            "  3.65786507e-03 9.02236671e-01 9.89124690e-02 1.50986544e-02\n",
            "  4.40619351e-01 3.30290326e-01 9.71884850e-01 1.57900373e-01\n",
            "  9.76350585e-01 1.94930772e-01 2.39933132e-02 2.40353343e-01\n",
            "  2.16602982e-01 1.10607204e-01 3.53475348e-01 1.66314906e-01\n",
            "  6.97514273e-01 9.62603146e-01 9.54543915e-01 3.47272820e-01\n",
            "  4.57822638e-01 4.79242697e-02 1.00467267e-02 2.00012480e-01\n",
            "  5.58034808e-01 7.67291256e-01 6.24890524e-01 2.03831574e-01\n",
            "  4.61450966e-01 1.19745643e-01 1.72766581e-01 2.48760046e-01\n",
            "  8.68152262e-03 1.53343578e-01 9.99195436e-01 6.39565905e-02\n",
            "  2.15048182e-02 8.09307150e-01 8.97467279e-01 5.97382839e-01\n",
            "  9.96450880e-01 9.97130054e-01 8.76569229e-01 2.86288715e-02\n",
            "  4.23223290e-01 2.60133578e-01 9.89533843e-01 1.30614248e-01\n",
            "  6.56519837e-01 1.22093780e-02 7.29515638e-01 9.87017917e-01\n",
            "  3.50080385e-01 1.44790969e-01 8.61919322e-01 1.22071282e-01\n",
            "  1.30428321e-01 9.16036491e-01 2.32651542e-01 1.76381773e-01\n",
            "  2.75578639e-01 6.31927479e-01 4.01328929e-02 1.70516715e-01\n",
            "  4.79014450e-01 3.49276511e-03 2.62993529e-01 9.36771457e-01\n",
            "  1.22197352e-01 9.85066784e-01 9.01486120e-01 6.50458050e-01\n",
            "  5.43333877e-01 9.98492845e-01 3.44325959e-02 9.92318379e-01\n",
            "  9.68440291e-01 4.26298680e-01 9.19246241e-02 9.74326614e-01\n",
            "  9.95979909e-01 4.26293273e-01 8.08243759e-01 5.31624011e-01\n",
            "  3.05390913e-01 9.24120054e-01 4.79727458e-01 5.73525019e-01\n",
            "  9.53137338e-01 4.52659530e-01 9.55695262e-01 9.08759280e-01\n",
            "  5.79846220e-01 9.80938122e-01 1.54462569e-02 9.93244136e-01\n",
            "  8.99628892e-01 4.20177826e-01 7.31549921e-03 9.64643166e-03\n",
            "  9.06573846e-01 9.80748009e-01 9.97293423e-01 7.53353176e-01\n",
            "  5.82123155e-01 8.71563972e-01 8.61590577e-01 2.42623146e-02\n",
            "  2.06909922e-01 2.38661201e-01 9.70385319e-01 7.83216909e-01\n",
            "  1.04673247e-03 3.98233602e-02 2.07024199e-02 3.44506664e-02\n",
            "  9.98767916e-01 8.96334582e-01 1.68129070e-01 9.98438765e-01\n",
            "  8.37692285e-01 3.15914098e-02 9.95379033e-01 9.00009385e-01\n",
            "  8.36195842e-01 5.92613108e-01 3.79004178e-01 3.19137265e-01\n",
            "  2.88766743e-01 3.59968124e-01 7.28110133e-02 9.86199237e-01\n",
            "  7.79898028e-01 5.98437933e-02 1.62995855e-01 1.81841034e-01\n",
            "  1.16313066e-01 9.50176445e-01 8.80808742e-01 3.81044449e-02\n",
            "  3.55933463e-02 9.76522712e-01 1.81085330e-02 2.57903710e-02\n",
            "  4.74465664e-01 9.36591003e-01 1.15616758e-01 1.62453014e-03\n",
            "  8.04038495e-01 9.43708821e-02 9.63792992e-01 3.46013032e-02\n",
            "  4.08405820e-02 9.65067497e-01 9.11130141e-01 6.52796182e-02\n",
            "  2.69843391e-01 3.43467924e-01 5.56784468e-02 2.00457152e-01\n",
            "  7.74240147e-03 4.66229013e-01 9.99570943e-01 9.90006128e-01\n",
            "  9.94191468e-01 6.51387031e-01 1.70308853e-01 2.42224991e-03\n",
            "  9.96360124e-01 9.38258523e-01 1.35414379e-02 8.83104180e-01\n",
            "  1.99894708e-01 2.93720040e-02 2.95616117e-01 6.71613902e-01\n",
            "  9.97069517e-01 2.46164998e-01 6.92732102e-02 9.40673063e-01\n",
            "  4.51155982e-01 8.34088647e-01 1.25942137e-02 3.78697128e-01\n",
            "  9.96752350e-01 8.52001489e-03 2.26543850e-01 6.05838957e-01\n",
            "  5.49236267e-01 3.03922706e-02 9.96711708e-01 7.40616077e-02\n",
            "  3.85638992e-01 1.30655037e-01 4.78624709e-04 6.37956749e-01\n",
            "  9.81149287e-01 9.96045361e-01 4.67871062e-01 1.07925665e-01\n",
            "  1.27432521e-01 9.94063021e-01 1.78186390e-01 2.54904289e-01\n",
            "  2.07154214e-03 9.43615212e-01 9.68413892e-01 2.09576082e-02\n",
            "  9.52175880e-01 9.56314923e-03 2.82802319e-02 4.95015720e-01\n",
            "  9.01282016e-01 9.95203346e-01 8.00243238e-01 1.25825458e-01\n",
            "  1.09367702e-01 2.20788776e-01 9.98525604e-01 1.96487797e-01\n",
            "  8.98214851e-01 9.99067714e-01 9.37667730e-01 2.74404733e-01\n",
            "  8.61164069e-01 1.76830319e-02 9.81334689e-01 8.66567090e-01\n",
            "  9.92936211e-01 4.06321259e-02 9.95848044e-01 9.97942583e-03\n",
            "  9.95541497e-01 1.12359710e-02 8.67389912e-02 9.75576449e-01\n",
            "  3.14337856e-01 9.65003514e-01 1.54055069e-02 5.39249028e-01\n",
            "  4.19922955e-02 2.21517002e-01 8.16690889e-01 4.82737476e-01\n",
            "  9.96418402e-01 9.04675019e-01 9.40762939e-01 9.81952649e-01\n",
            "  2.67219967e-04 9.02011582e-01 9.97461283e-01 1.49888919e-01\n",
            "  3.08166959e-01 1.05012256e-02 2.34647863e-01 9.94433288e-01\n",
            "  9.87479825e-01 9.84353343e-01 2.68712610e-02 9.27005404e-01\n",
            "  3.39565528e-02 6.91486675e-01 1.47338563e-01 5.30664109e-01\n",
            "  6.98430847e-01 9.17997284e-01 2.11267284e-02 9.81028815e-01\n",
            "  4.44696472e-01 6.18329387e-03 1.97401873e-02 9.81130948e-01\n",
            "  9.99943805e-01 2.08293678e-01 9.84776686e-01 1.91366741e-01\n",
            "  9.71368695e-01 2.20176579e-01 1.34814451e-01 5.20228935e-01\n",
            "  3.81628226e-01 3.98662942e-01 9.99996150e-01 1.27528422e-01\n",
            "  9.94510112e-01 4.15756395e-01 6.56178457e-01 6.99230997e-02\n",
            "  2.97786656e-01 9.88061598e-03 4.72179762e-01 1.49982166e-02\n",
            "  7.37992211e-01 8.54132458e-02 9.74237035e-01 6.44968486e-02\n",
            "  8.45038052e-01 1.53520741e-01 1.00211999e-01 6.73782074e-02\n",
            "  9.51773539e-01 8.39061171e-01 1.70326376e-01 9.98696530e-01\n",
            "  8.43258179e-01 7.78286336e-03 9.97400439e-01 6.63109832e-01\n",
            "  1.51670813e-01 9.09988345e-02 7.28398663e-01 9.49839202e-01\n",
            "  5.46900430e-02 9.88681921e-01 9.40013708e-01 5.24580089e-02\n",
            "  9.82245193e-01 6.43821530e-02 9.99216565e-01 9.72815138e-01\n",
            "  6.34838883e-01 9.65978315e-01 9.96700948e-01 9.99558368e-01\n",
            "  6.14498255e-02 9.99802244e-01 5.45027602e-01 5.10392403e-01\n",
            "  6.99710439e-01 9.09663695e-01 8.86878108e-01 7.24873073e-01\n",
            "  3.96964571e-03 3.48792580e-01 2.80762337e-03 1.80663366e-02\n",
            "  9.41018375e-01 9.70721154e-01 1.03705718e-02 9.97730325e-01\n",
            "  8.56100314e-01 5.76475943e-01 9.99545289e-01 1.43862981e-01\n",
            "  9.93741943e-01 3.25507123e-03 8.77530981e-01 9.13838012e-01\n",
            "  8.36043588e-01 8.01936596e-04 1.58314020e-01 9.60174148e-01\n",
            "  1.96590475e-01 9.78894090e-01 5.68645834e-01 8.32534708e-01\n",
            "  8.95454527e-01 9.90172744e-01 2.11125942e-01 1.32306372e-02\n",
            "  1.27052933e-02 4.61237607e-01 9.74952639e-01 1.08404491e-01\n",
            "  4.08639591e-02 2.63879071e-01 9.95059905e-01 3.12239709e-01\n",
            "  1.50761608e-02 3.65858382e-01 9.97905677e-01 7.16455500e-01\n",
            "  9.99909207e-02 9.05444780e-01 1.78736952e-02 9.78860898e-01\n",
            "  9.01150143e-03 8.85871054e-01 9.84350510e-01 2.57334801e-01\n",
            "  1.75046078e-02 8.28340627e-01 5.47288880e-01 9.62332684e-01\n",
            "  7.97203241e-01 1.16008128e-01 9.34099723e-01 9.98345613e-01\n",
            "  2.04814752e-02 1.74215212e-01 9.95203985e-01 6.94870581e-02\n",
            "  9.97560932e-01 9.76997155e-01 6.71522968e-01 1.92416776e-02\n",
            "  9.99846448e-01 1.92023123e-02 6.85258070e-01 2.08443883e-01\n",
            "  2.30791251e-02 9.72311901e-01 1.44004841e-01 7.95355545e-01\n",
            "  7.57891570e-01 3.61733233e-01 2.06777555e-01 9.03774308e-02\n",
            "  1.73751995e-02 9.67984591e-01 3.06322892e-01 1.15894739e-02\n",
            "  1.08553791e-02 3.80755369e-01 1.91958815e-02 1.34946519e-02\n",
            "  4.23236293e-01 2.22001571e-03 1.43245816e-02 8.56885365e-01\n",
            "  6.92307012e-02 8.85156379e-01 9.78924054e-01 2.39643058e-01\n",
            "  3.46328101e-02 7.59303591e-02 2.11126025e-02 4.86473303e-01\n",
            "  1.11169629e-02 1.80936361e-03 5.44124263e-01 2.97929909e-02\n",
            "  7.78763971e-02 1.67361132e-01 4.64007197e-01 2.80046328e-01\n",
            "  9.98378500e-01 2.89196023e-01 5.68218520e-01 6.27592135e-02\n",
            "  9.99403097e-01 4.87108608e-01 4.38219726e-02 9.65476050e-01\n",
            "  9.91862345e-01 2.67214875e-02 8.16843076e-01 9.87868749e-01\n",
            "  8.77698452e-03 7.43354094e-01 4.46760221e-01 3.13916021e-01\n",
            "  9.73521324e-01 4.22747115e-02 9.99443415e-01 9.08671186e-01\n",
            "  2.75598820e-02 1.09623982e-01 3.33684850e-02 7.10258076e-01\n",
            "  2.52857158e-01 9.95266761e-01 1.39122292e-02 6.69223008e-01\n",
            "  9.96177312e-01 1.18716507e-01 6.90458274e-01 9.90098083e-01\n",
            "  1.45947816e-01 2.29208636e-01 5.44726599e-02 8.17807808e-01\n",
            "  3.88809598e-01 6.66095052e-01 3.54737206e-01 9.29846657e-01\n",
            "  9.60895744e-02 1.20387195e-02 8.32804055e-01 8.67473835e-01\n",
            "  1.07553780e-02 9.95679126e-01 2.99954919e-02 5.56105707e-01\n",
            "  5.23957115e-02 9.98053539e-01 9.35561073e-01 9.90373324e-01\n",
            "  6.37402211e-01 8.09319579e-01 2.38386454e-03 5.58895862e-03\n",
            "  9.98396002e-01 1.31396101e-01 3.03709210e-01 2.28662645e-01\n",
            "  5.85812574e-01 9.76892090e-01 2.63233942e-01 3.98538327e-02\n",
            "  8.15475711e-04 3.16645143e-01 7.73946826e-01 3.03967878e-03\n",
            "  1.48016286e-01 9.95928688e-01 9.37869027e-01 9.38638267e-01\n",
            "  8.46409999e-01 9.86827342e-01 9.55585462e-01 5.77379462e-01\n",
            "  1.16669083e-01 4.02254362e-01 8.84872482e-01 9.73015871e-01\n",
            "  3.37153258e-01 8.96785331e-01 5.06093397e-02 2.87575176e-01\n",
            "  4.71925519e-01 8.49652375e-02 9.99893034e-01 5.80328720e-02\n",
            "  7.16911434e-04 3.00981910e-02 4.74421351e-01 8.71591742e-01\n",
            "  9.85292455e-01 9.79419654e-01 4.28767050e-02 1.46532496e-01\n",
            "  8.85491392e-02 2.59487068e-01 3.22482982e-01 1.35564421e-01\n",
            "  8.94420784e-01 5.64515886e-01 2.40410566e-01 7.08259626e-01\n",
            "  9.77358640e-01 2.97587072e-01 9.91775538e-01 9.46394681e-01\n",
            "  4.15117001e-01 3.24700235e-01 9.19676260e-01 2.50807260e-02\n",
            "  9.77515938e-01 2.15363445e-01 2.33404036e-01 8.63840453e-02\n",
            "  5.07290997e-01 8.04848478e-01 1.99448972e-01 6.18223882e-01\n",
            "  1.00537356e-02 2.44890738e-02 4.79434618e-02 5.10552217e-01\n",
            "  9.39349193e-01 9.85765060e-01 8.81782843e-01 5.69784510e-01\n",
            "  8.09787846e-01 4.46113140e-01 3.40265648e-01 9.24293486e-01\n",
            "  1.42311591e-01 4.52818595e-03 1.17635808e-02 9.95346107e-01\n",
            "  6.65411384e-01 4.13528442e-01 2.99213769e-01 9.59359945e-01\n",
            "  6.72206137e-01 7.72246971e-01 7.28069845e-01 1.51680859e-02\n",
            "  4.16642438e-01 1.87327214e-01 2.14496180e-02 2.11955340e-01\n",
            "  9.76998846e-01 7.82309467e-01 8.23185636e-01 4.85552207e-02\n",
            "  6.33055926e-01 2.52524876e-01 5.48187308e-01 6.15620900e-01\n",
            "  9.92503701e-01 6.92131883e-01 8.63047087e-01 9.76006822e-01\n",
            "  5.81553857e-01 1.36780948e-01 6.89760095e-01 2.93044735e-02\n",
            "  1.00287480e-01 9.55864939e-01 5.26006648e-01 3.47895670e-01\n",
            "  9.68914330e-01 1.18908437e-02 5.24429248e-02 8.37352101e-01\n",
            "  3.75660360e-01 6.30458799e-01 1.26412700e-01 5.86042160e-01\n",
            "  3.99167156e-01 9.98788495e-01 2.37016577e-02 5.16875559e-02\n",
            "  8.30804567e-01 9.98280193e-01 9.69286857e-01 9.95505589e-01\n",
            "  1.20562843e-02 1.95518449e-02 9.94353297e-01 5.31777273e-02\n",
            "  1.36364075e-02 9.98455764e-01 1.13908033e-01 9.95844827e-01\n",
            "  5.22592123e-02 1.22275875e-01 2.16317127e-01 5.30549536e-01\n",
            "  8.45139765e-01 6.96808726e-01 9.83261262e-01 2.15742222e-01\n",
            "  9.54740836e-01 3.68514575e-01 1.77171026e-01 9.66537678e-01\n",
            "  1.30694371e-01 1.07708932e-02 2.84079729e-01 7.31456220e-01\n",
            "  9.48797410e-01 9.43998207e-01 1.03873110e-02 4.87774769e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7848JgildZEa"
      },
      "source": [
        "### 4. Создайте функцию calc_pred, возвращающую предсказанный класс (на вход подаются W, который уже посчитан функцией eval_model и X, на выходе - массив y_pred)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w60YyJlhdp9k"
      },
      "source": [
        "# Функция для выполения предсказаний\n",
        "def predict(w, X):\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    \n",
        "    y_predicted = np.zeros((1, m))\n",
        "    w = w.reshape(X.shape[0], 1)\n",
        "    \n",
        "    A = sigmoid(np.dot(w.T, X))\n",
        "    \n",
        "    # За порог отнесения к тому или иному классу примем вероятность 0.5\n",
        "    for i in range(A.shape[1]):\n",
        "        if (A[:,i] > 0.5): \n",
        "            y_predicted[:, i] = 1\n",
        "        elif (A[:,i] <= 0.5):\n",
        "            y_predicted[:, i] = 0\n",
        "    \n",
        "    return y_predicted"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdlBlEGUdjNw",
        "outputId": "e97fda67-1547-4d55-cd9d-03cae16eaa7d"
      },
      "source": [
        "y_pred = predict(w, X_tr)\n",
        "y_pred"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
              "        1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
              "        0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
              "        0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
              "        0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
              "        0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
              "        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
              "        0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
              "        0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
              "        0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
              "        1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
              "        0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0.,\n",
              "        0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
              "        0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
              "        1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
              "        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "        1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
              "        1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
              "        0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
              "        1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
              "        0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
              "        0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
              "        1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
              "        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
              "        1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
              "        1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
              "        0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
              "        0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
              "        0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
              "        1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
              "        0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
              "        1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
              "        1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
              "        0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
              "        1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
              "        1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
              "        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
              "        1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
              "        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
              "        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
              "        0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
              "        0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
              "        1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
              "        0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
              "        0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
              "        1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
              "        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
              "        1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
              "        1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
              "        0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
              "        1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
              "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
              "        0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
              "        0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
              "        0., 0., 0., 1., 1., 1., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZS_9Y6hyhW0X"
      },
      "source": [
        "### 5. Посчитайте Accuracy, матрицу ошибок, точность и полноту, а также F1 score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdhpOFQ2hWL4",
        "outputId": "f816a6f3-d770-4ed7-8599-dc9cd7c22a44"
      },
      "source": [
        "# В качестве меры точности возьмем долю правильных ответов\n",
        "accuracy = 100.0 - np.mean(np.abs(y_pred - y_tr)*100.0)\n",
        "print(f\"Точность: {accuracy:.3f}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Точность: 86.800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXR92xj6lG0a"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Функция расчета метрик\n",
        "def confusion_matrix_1(y, y_pred):\n",
        "    tp, tn, fp, fn = 0, 0, 0, 0\n",
        "    for i in range(len(y)):\n",
        "        tp += 1 if y[i] == y_pred[i] and y[i] == 1 else 0\n",
        "        tn += 1 if y[i] == y_pred[i] and y[i] == 0 else 0\n",
        "        fp += 1 if y[i] != y_pred[i] and y[i] == 0 else 0\n",
        "        fn += 1 if y[i] != y_pred[i] and y[i] == 1 else 0\n",
        "   \n",
        "    matrix = pd.DataFrame({'y=1': [tp, fp],\n",
        "                          'y=0': [fn, tn]}, \n",
        "                          index=['y_pred=1', 'y_pred=0'])\n",
        "    \n",
        "    return matrix, tp, tn, fp, fn"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDMj2EAvly9p",
        "outputId": "b020a53f-bd6c-4d9b-abeb-9a2abd51cc1d"
      },
      "source": [
        "# Для тестирования посчитаем метрики при помощи библиотек\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "print(f\"f1 = {f1_score(y_tr[0], y_pred[0])}, prec = {precision_score(y_tr[0], y_pred[0])}, recall = {recall_score(y_tr[0], y_pred[0])}\")\n",
        "confusion_matrix(y_tr[0], y_pred[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1 = 0.8639175257731959, prec = 0.8933901918976546, recall = 0.8363273453093812\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[449,  50],\n",
              "       [ 82, 419]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "fvYtnfagkZcn",
        "outputId": "f512e1c2-b9e4-425e-d8c3-1949f1e15f97"
      },
      "source": [
        "matrix, tp, tn, fp, fn = confusion_matrix_1(y_tr[0], y_pred[0])\n",
        "\n",
        "prec = tp/(tp+fp)\n",
        "recall = tp/(tp+fn)\n",
        "f1= 2 * prec * recall / (prec + recall)\n",
        "\n",
        "print(f\"f1 = {f1}, prec = {prec}, recall = {recall}\")\n",
        "matrix"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1 = 0.8639175257731959, prec = 0.8933901918976546, recall = 0.8363273453093812\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y=1</th>\n",
              "      <th>y=0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>y_pred=1</th>\n",
              "      <td>419</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>y_pred=0</th>\n",
              "      <td>50</td>\n",
              "      <td>449</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          y=1  y=0\n",
              "y_pred=1  419   82\n",
              "y_pred=0   50  449"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK0Gqd9zvXFw"
      },
      "source": [
        "### 6. Могла ли модель переобучиться? Почему?\n",
        "\n",
        "Модель могла переобучиться в силу специфики логарифмической функции ошибок, которая упирается в бескоечности не имеет минимумов. Модель бесконечно будет корректировать веса уменьшая ошибку и все больше подгонять ответ под данные с увеличением количесвта итераций или уменьшения скорости обучения."
      ]
    }
  ]
}